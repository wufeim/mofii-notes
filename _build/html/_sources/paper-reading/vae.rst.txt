Auto-Encoding Variational Bayes
=====================================

| **Authors:** Diederik P. Kingma, Max Welling
| **Affiliations:** Universiteit van Amsterdam

In this work, the authors propose a stochastic variational inference and learning algorithm that performs efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions and large datasets.

Their contributions is two-fold:
  1. They show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods.
  2. They show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model to the intractable posterior using the proposed lower bound estimator.

When a neural network is used for the recognition model, we arrive at the **variational auto-encoder**.

Method
-------------------------------------

The strategy in this section can be used to derive a lower bound estimator (a stochastic objective function) for a variety of directed graphical models with continuous latent variables. We will restrict ourselves here to the common case where we have an i.i.d. dataset with latent variables per datapoint, and where we like to perform maximum likelihood (ML) or maximum a posteriori (MAP) inference on the (global) parameters, and variational inference on the latent variables.

Problem Scenario
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Let us consider some dataset :math:`\mathbf{X} = \{\mathbf{x}^{(i)}\}_{i=1}^N` consisting of :math:`N` i.i.d. samples. We assume that the data are generated by some random process, involving an unobserved continuous random variable :math:`\mathbf{z}`. This process consists of two steps:
  1. a value :math:`\mathbf{z}^{(i)}` is generated from some prior distribution :math:`p_{\mathbf{\theta}^*}(\mathbf{z})`
  2. a value :math:`\mathbf{x}^{(i)}` is generated from some conditional distribution :math:`p_{\mathbf{\theta}^*}(\mathbf{x} \mid \mathbf{z})`
We assume that the prior :math:`p_{\mathbf{\theta}^*}(\mathbf{z})` and likelihood :math:`p_{\mathbf{\theta}^*}(\mathbf{x} \mid \mathbf{z})` come from parametric families of distributions :math:`p_\mathbf{\theta}(\mathbf{z})` and :math:`p_\mathbf{\theta}(\mathbf{x} \mid \mathbf{z})`, and their PDFs are differentiable almost everywhere w.r.t. both :math:`\mathbf{\theta}` and :math:`\mathbf{z}`.

Example: Variation Auto-Encoder
-------------------------------------

Experiments
-------------------------------------

Conclusion
-------------------------------------

Future Work
-------------------------------------
