An End-to-End Learning Framework for Video Compression
=====================================

| **Year:** Apr 2020
| **Authors:** Guo Lu, Xiaoyun Zhang, Wanli Ouyang, Li Chen, Zhiyong Gao, Dong Xu
| **Links:** `IEEE Xplore <https://ieeexplore.ieee.org/document/9072487>`_

Existing methods try to employ DNN for video compression [1]. However, most work only replace one or two modules in the traditional framework instead of optimizing the video compression system in an end-to-end fashion.

There are two challenges for building an end-to-end optimized video compression system:
    - it is very difficult to build a learning based video compression system because of the complicated coding procedure
    - it is necessary to design a scheme to generate and compress the motion information that is tailored for video compression

Inspired by the previous work [2], the authors propose the first end-to-end deep video compression (DVC) framework. All components in video compression are implemented with the end-to-end neural networks, and are jointly optimized based on the rate-distortion trade-off through a single loss function. The framework is also very flexible and two variants, :code:`DVC_Lite` and :code:`DVC_Pro`, are proposed for speed/efficiency priority. The adaptive quantization layer for the DVC framework significantly reduces the number of parameters for variable bitrate coding.

Expreimental results show that the proposed approach can outperform the widely used video coding standard H.264 and be even on par with the latest standard H.265.

Proposed Framework
-------------------------------------

Notations:
    - :math:`\mathcal{V} = \{x_1, \dots, x_{t-1}, x_t, \dots \}`: current video sequences
    - :math:`\bar{x}_t`: predicted frame
    - :math:`\hat{x}_t`: reconstructed frame
    - :math:`r_t` (transformed to :math:`y_t`): residual (error) between the original frame :math:`x_t` and the predicted frame :math:`\bar{x}_t`
    - :math:`\hat{r}_t`: residual (error) between the original frame :math:`x_t` and the reconstructed/decoded frame :math:`\hat{x}_t`
    - :math:`v_t` (transformed to :math:`m_t`): motion vector or optical flow value
    - :math:`\hat{v}_t`: reconstructed version

Let :math:`\mathcal{V} = \{x_1, \dots, x_{t-1}, x_t, \dots \}` denote the current video sequences. The predicted frame is denoted as :math:`\bar{x}_t` and the reconstructed/decoded frame is denoted as :math:`\hat{x}_t`. :math:`r_t` represents the residual (error) between the original frame :math:`x_t` and the predicted frame :math:`\bar{x}_t`. :math:`\hat{r}_t` represents the reconstructed/decoded residual. :math:`v_t` represents the 

Reference
-------------------------------------

**[1]** Liu, D., Li, Y., Lin, J., Li, H., & Wu, F. (2020). Deep learning-based video coding: A review and a case study. ACM Computing Surveys (CSUR), 53(1), 1-35.

**[2]** Lu, G., Ouyang, W., Xu, D., Zhang, X., Cai, C., & Gao, Z. (2019). Dvc: An end-to-end deep video compression framework. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 11006-11015).
