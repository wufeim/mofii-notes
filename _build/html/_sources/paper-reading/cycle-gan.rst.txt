Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks
=====================================

| **Year:** Mar 2017
| **Authors:** Jun-Yan Zhu, Taesung Park, Phillip Isola, Alexei A. Efros
| **Affiliations:** Berkeley AI Research Laboratory

In this work, the authors present an approach for learning to translate an image from a source domain :math:`X` to a target domain :math:`Y` in the absence of paired examples :math:`\{x_i, y_i\}_{i=1}^N`.

They train a mapping :math:`G: X \to Y` such that the output :math:`\hat{y} = G(x)`, :math:`x \in X` is indistinguishable from images :math:`y \in Y`. However, such a translation does not guarantee that an individual input :math:`x` and output :math:`y` are paired up in a meaningful way. Moreover, standard procedures often lead to the problem of **mode collapse**.

Therefore, they exploit the property that the translation should be "cycle consistent". If we have a translator :math:`G: X \to Y` and another translator :math:`F: Y \to X`, then :math:`G` and :math:`F` should be inverses of each other and both mappings should be bijections. They apply this structural assumption by training :math:`G` and :math:`F` simultaneously and adding a **cycle consistency loss** that encourages :math:`F(G(x)) \approx x` and :math:`G(F(y)) \approx y`.

Adversarial Loss
-------------------------------------

Adversarial losses are applied to both mapping functions. For the mapping function :math:`G: X \to Y` and its discriminator :math:`D_Y`, the objective is expressed as:

.. math::

   \mathcal{L}_\text{GAN}(G, D_Y, X, Y) = \mathbb{E}_{y \sim p_\text{data}(y)}[\log D_Y(y)] + \mathbb{E}_{x \sim p_\text{data}(x)}[\log (1 - D_Y(G(x)))]
