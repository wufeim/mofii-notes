
<!DOCTYPE html>

<html lang="cn">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs (Pix2PixHD) &#8212; Mofii Notes 1.0.1 documentation</title>
    <link rel="stylesheet" href="../_static/classic.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Image Based Virtual Try-On Network from Unpaired Data" href="virtual-try-on-from-unpaired-data.html" />
    <link rel="prev" title="Image-to-Image Translation with Conditional Adversarial Networks (Pix2Pix)" href="pix2pix.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="virtual-try-on-from-unpaired-data.html" title="Image Based Virtual Try-On Network from Unpaired Data"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="pix2pix.html" title="Image-to-Image Translation with Conditional Adversarial Networks (Pix2Pix)"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">Mofii Notes 1.0.1 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../paper-reading.html" >Paper Reading Notes</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="../generative.html" accesskey="U">Generative Models</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs (Pix2PixHD)</a></li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="high-resolution-image-synthesis-and-semantic-manipulation-with-conditional-gans-pix2pixhd">
<h1>High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs (Pix2PixHD)<a class="headerlink" href="#high-resolution-image-synthesis-and-semantic-manipulation-with-conditional-gans-pix2pixhd" title="Permalink to this headline">¶</a></h1>
<div class="line-block">
<div class="line"><strong>Authors:</strong> Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, Bryan Catanzaro</div>
<div class="line"><strong>Affiliations:</strong> NVIDIA Corporation, UC Berkeley</div>
</div>
<p>In this paper, the authors discuss a new approach that produces high-resolution (2048 x 1024) images from semantic label maps. They address two main issues of previous SOTA methods:</p>
<ul class="simple">
<li><p>the difficulty of generating high-resolution images with GANs</p></li>
<li><p>the lack of details and realistic textures</p></li>
</ul>
<p>They propose to generate 2048 by 1024 visually appealing results with a novel adversarial loss, as well as new multi-scale generator and discriminator architectures.</p>
<p>They further extend the method in two directions:</p>
<ul class="simple">
<li><p>enable flexible object manipulation using instance-level object segmentation information</p></li>
<li><p>generate diverse results given the same input label map</p></li>
</ul>
<p>This method outperforms previous approaches regarding quantitative evaluations and human perception studies.</p>
<div class="section" id="improving-photorealism-and-resolution">
<h2>Improving Photorealism and Resolution<a class="headerlink" href="#improving-photorealism-and-resolution" title="Permalink to this headline">¶</a></h2>
<p>The authors improve the Pix2Pix framework by using a coarse-to-fine generator, a multi-scale discriminator architecture, and a robust adversarial learning objective function.</p>
<a class="reference internal image-reference" href="../_images/high-res-img-syth-semantic-manipulation-cgan-1.png"><img alt="../_images/high-res-img-syth-semantic-manipulation-cgan-1.png" src="../_images/high-res-img-syth-semantic-manipulation-cgan-1.png" style="width: 520pt;" /></a>
<p><strong>Coarse-to-fine generator:</strong> The authors decompose the generator into two sub-networks: the global generator network <span class="math notranslate nohighlight">\(G_1\)</span> and the local enhancer <span class="math notranslate nohighlight">\(G_2\)</span>. The global generator network outputs at a resolution of 1024 x 512, and the local enhancer outputs with a resolution of 2048 x 1024. Additional local enhancer networks could be utilized to synthesize images at an higher resolution.</p>
<p>The global generator is built on the architecture proposed in <strong>[1]</strong>. It consists of 3 components: a convolutional front-end <span class="math notranslate nohighlight">\(G_1^{(F)}\)</span>, a set of residual blocks <span class="math notranslate nohighlight">\(G_1^{(R)}\)</span>, and a transposed convolutional back-end <span class="math notranslate nohighlight">\(G_1^{(B)}\)</span>.</p>
<p>The local enhancer network also consists of 3 components: a convolutional front-end <span class="math notranslate nohighlight">\(G_2^{(F)}\)</span>, a set of residual blocks <span class="math notranslate nohighlight">\(G_2^{(R)}\)</span>, and a transposed convolutional back-end <span class="math notranslate nohighlight">\(G_2^{(B)}\)</span>. The input to <span class="math notranslate nohighlight">\(G_2^{(R)}\)</span> is the element-wise sum of the <span class="math notranslate nohighlight">\(G_2^{(F)}\)</span> and <span class="math notranslate nohighlight">\(G_1^{(B)}\)</span>.</p>
<p>During training, they first train <span class="math notranslate nohighlight">\(G_1\)</span> on lower resolution images, and then train the two networks jointly on high resolution images.</p>
<p><strong>Multi-scale discriminators:</strong> The authors use 3 discriminators that have an identical network structure but operate at different image scales, referred to as <span class="math notranslate nohighlight">\(D_1\)</span>, <span class="math notranslate nohighlight">\(D_2\)</span>, and <span class="math notranslate nohighlight">\(D_3\)</span>. <strong>Without the mutli-scale discriminators, the authors observe that many repeated patterns often appear in the generated images.</strong></p>
<p><strong>Improved adversarial loss:</strong> The authors improve the GAN loss by incorporating a feature matching loss based on the discriminator. Specifically, they extract features from multiple layers of the discriminator and learn to match these intermediate representations from the real and the synthesized images. The feature matching loss <span class="math notranslate nohighlight">\(\mathcal{L}_{FM}(G, D_k)\)</span> is calculated as:</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}_{FM}(G, D_k) = \mathbb{E}_{(\mathbf{s}, \mathbf{x})} \frac{1}{N_i} \left[ \left\lVert D_k^{(i)}(\mathbf{s}, \mathbf{x}) - D_k^{(i)}(\mathbf{s}, G(\mathbf{s})) \right\rVert_1 \right]\]</div>
<p>The full objective combines both GAN loss and feature matching loss as:</p>
<div class="math notranslate nohighlight">
\[\min_G \left( \left( \max_{D_1, D_2, D_3} \sum_{k=1, 2, 3} \mathcal{L}_{GAN}(G, D_k) \right) + \lambda \sum_{k=1, 2, 3} \mathcal{L}_{FM}(G, D_k) \right)\]</div>
</div>
<div class="section" id="using-instance-maps">
<h2>Using Instance Maps<a class="headerlink" href="#using-instance-maps" title="Permalink to this headline">¶</a></h2>
<p>The authors argue that the most critical information the instance map provides is the object boundary. A pixel in the instance boundary map is 1 if its object ID is different from any of its 4-neighbors, and 0 otherwise. The input to the networks is the channel-wise concatenation of instance boundary map, semantic label map, and the image.</p>
<a class="reference internal image-reference" href="../_images/high-res-img-syth-semantic-manipulation-cgan-2.png"><img alt="../_images/high-res-img-syth-semantic-manipulation-cgan-2.png" src="../_images/high-res-img-syth-semantic-manipulation-cgan-2.png" style="width: 360pt;" /></a>
<a class="reference internal image-reference" href="../_images/high-res-img-syth-semantic-manipulation-cgan-3.png"><img alt="../_images/high-res-img-syth-semantic-manipulation-cgan-3.png" src="../_images/high-res-img-syth-semantic-manipulation-cgan-3.png" style="width: 360pt;" /></a>
</div>
<div class="section" id="learning-an-instance-level-feature-embedding">
<h2>Learning an Instance-Level Feature Embedding<a class="headerlink" href="#learning-an-instance-level-feature-embedding" title="Permalink to this headline">¶</a></h2>
<p>To generate diverse images and allow instance-level control, the authors propose adding additional low-dimensional feature channels as the input to the generator network.</p>
<p>They train an encoder network <span class="math notranslate nohighlight">\(E\)</span> to find a low-dimensional feature vector that corresopnds to the ground truth target for each instance in the image. An instance-wise average pooling layer is used to compute the average feature for the object instance. The average feature is then broadcast to all the pixel locations of the instance.</p>
<a class="reference internal image-reference" href="../_images/high-res-img-syth-semantic-manipulation-cgan-4.png"><img alt="../_images/high-res-img-syth-semantic-manipulation-cgan-4.png" src="../_images/high-res-img-syth-semantic-manipulation-cgan-4.png" style="width: 360pt;" /></a>
</div>
<div class="section" id="results">
<h2>Results<a class="headerlink" href="#results" title="Permalink to this headline">¶</a></h2>
<p>To quantify the quality of the results, the authors perform semantic segmentation on the synthesized images and compare how well the predicted segments match the input.</p>
<p>They further evalute the algorithm via a human subjective study.</p>
<a class="reference internal image-reference" href="../_images/high-res-img-syth-semantic-manipulation-cgan-5.png"><img alt="../_images/high-res-img-syth-semantic-manipulation-cgan-5.png" src="../_images/high-res-img-syth-semantic-manipulation-cgan-5.png" style="width: 360pt;" /></a>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<p><strong>[1]</strong> Johnson, J., Alahi, A., &amp; Fei-Fei, L. (2016, October). Perceptual losses for real-time style transfer and super-resolution. In European conference on computer vision (pp. 694-711). Springer, Cham.</p>
</div>
</div>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="../index.html">Table of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs (Pix2PixHD)</a><ul>
<li><a class="reference internal" href="#improving-photorealism-and-resolution">Improving Photorealism and Resolution</a></li>
<li><a class="reference internal" href="#using-instance-maps">Using Instance Maps</a></li>
<li><a class="reference internal" href="#learning-an-instance-level-feature-embedding">Learning an Instance-Level Feature Embedding</a></li>
<li><a class="reference internal" href="#results">Results</a></li>
<li><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="pix2pix.html"
                        title="previous chapter">Image-to-Image Translation with Conditional Adversarial Networks (Pix2Pix)</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="virtual-try-on-from-unpaired-data.html"
                        title="next chapter">Image Based Virtual Try-On Network from Unpaired Data</a></p>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../_sources/paper-reading/high-res-img-syth-semantic-manipulation-cgan.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="virtual-try-on-from-unpaired-data.html" title="Image Based Virtual Try-On Network from Unpaired Data"
             >next</a> |</li>
        <li class="right" >
          <a href="pix2pix.html" title="Image-to-Image Translation with Conditional Adversarial Networks (Pix2Pix)"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">Mofii Notes 1.0.1 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../paper-reading.html" >Paper Reading Notes</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="../generative.html" >Generative Models</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs (Pix2PixHD)</a></li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2020, Mofii.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 3.1.2.
    </div>
  </body>
</html>