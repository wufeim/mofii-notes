
<!DOCTYPE html>

<html lang="cn">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Focal Loss for Dense Object Detection (RetinaNet) &#8212; Mofii Notes 1.0.1 documentation</title>
    <link rel="stylesheet" href="../_static/classic.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="End-to-End Object Detection with Transformers (DETR)" href="detr.html" />
    <link rel="prev" title="Probabilistic Anchor Assignment with IoU Prediction for Object Detection" href="../paa.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="detr.html" title="End-to-End Object Detection with Transformers (DETR)"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="../paa.html" title="Probabilistic Anchor Assignment with IoU Prediction for Object Detection"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">Mofii Notes 1.0.1 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../paper-reading.html" >Paper Reading Notes</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="../obj-det.html" accesskey="U">Object Detection</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Focal Loss for Dense Object Detection (RetinaNet)</a></li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="focal-loss-for-dense-object-detection-retinanet">
<h1>Focal Loss for Dense Object Detection (RetinaNet)<a class="headerlink" href="#focal-loss-for-dense-object-detection-retinanet" title="Permalink to this headline">¶</a></h1>
<div class="line-block">
<div class="line"><strong>Authors:</strong> Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, Piotr Dollar</div>
<div class="line"><strong>Affiliations:</strong> Facebook AI Research</div>
</div>
<p>In this paper, the authors discover that the central cause why one-stage detectors have trailed the accuracy of two-stage detectors is the extreme foreground-backgruond class imbalance encountered during training of dense detectors. They propose <strong>Focal Loss</strong> that focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training.</p>
<p>To evaluate the effectiveness of the loss, they design and train a simple dense detector called <strong>RetinaNet</strong>. Experiments on the COCO dataset show that RetinaNet trained with focal loss achieves SOTA speed and accuracy.</p>
<div class="section" id="focal-loss">
<h2>Focal Loss<a class="headerlink" href="#focal-loss" title="Permalink to this headline">¶</a></h2>
<p>Let's first consider the (binary) cross entropy (CE) loss:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\text{CE}(p, y) = \begin{cases} -\log(p) &amp; \text{if } y = 1 \\ -\log(1-p) &amp; \text{otherwise} \end{cases}\end{split}\]</div>
<p>For notational convenience, we define <span class="math notranslate nohighlight">\(p_t\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}p_t = \begin{cases} p &amp; \text{if } y = 1 \\ 1 - p &amp; \text{otherwise} \end{cases}\end{split}\]</div>
<p>and rewrite the CE as</p>
<div class="math notranslate nohighlight">
\[\text{CE}(p, y) = \text{CE}(p_t) = -\log(p_t)\]</div>
<p>A simple extension is the <strong>balanced cross entropy</strong> which introduce a weighting factor <span class="math notranslate nohighlight">\(\alpha\)</span>. In practice <span class="math notranslate nohighlight">\(\alpha\)</span> may be set by inverse class frequency or treated as a hyperparameter to set by cross validation. We write the <span class="math notranslate nohighlight">\(\alpha\)</span>-balanced CE loss as:</p>
<div class="math notranslate nohighlight">
\[\text{CE}(p_t) = - \alpha_t \log(p_t)\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[\begin{split}\alpha_t = \begin{cases} \alpha &amp; \text{if } y = 1 \\ 1 - \alpha &amp; \text{otherwise} \end{cases}\end{split}\]</div>
<p>The <strong>focal loss</strong> is designed to address the extreme imbalance between foreground and background classes during training. While <span class="math notranslate nohighlight">\(\alpha\)</span> balances the importance of positive/negative examples, it does not differentiate between easy/hard examples. The authors propose to add a modulating factor <span class="math notranslate nohighlight">\((1 - p_t)^\gamma\)</span> to the cross entropy loss, with tunable <strong>focusing parameter</strong> <span class="math notranslate nohighlight">\(\gamma \geq 0\)</span>. The focal loss is defined as:</p>
<div class="math notranslate nohighlight">
\[\text{FL}(p_t) = - (1 - p_t)^\gamma \log(p_t)\]</div>
<a class="reference internal image-reference" href="../_images/retina-net-1.png"><img alt="../_images/retina-net-1.png" src="../_images/retina-net-1.png" style="width: 360pt;" /></a>
<dl class="simple">
<dt>There are two properties of focal loss:</dt><dd><ol class="arabic simple">
<li><p>As <span class="math notranslate nohighlight">\(p_t\)</span> goes to <span class="math notranslate nohighlight">\(1\)</span>, the factor <span class="math notranslate nohighlight">\((1 - p_t)^\gamma\)</span> goes to <span class="math notranslate nohighlight">\(0\)</span> and the loss is down-weighted.</p></li>
<li><p><span class="math notranslate nohighlight">\(\gamma\)</span> smoothly adjusts the rate at which easy examples are down-weighted. (The authors found <span class="math notranslate nohighlight">\(\gamma = 2\)</span> to work the best in experiments.)</p></li>
</ol>
</dd>
</dl>
<p>In practice they use an <span class="math notranslate nohighlight">\(\alpha\)</span>-balanced variant of the focal loss:</p>
<div class="math notranslate nohighlight">
\[\text{FL}(p_t) = -\alpha_t (1 - p_t)^\gamma \log(p_t)\]</div>
<p>The derivatives of the loss functions are plotted below:</p>
<a class="reference internal image-reference" href="../_images/retina-net-3.png"><img alt="../_images/retina-net-3.png" src="../_images/retina-net-3.png" style="width: 360pt;" /></a>
</div>
<div class="section" id="retinanet-detector">
<h2>RetinaNet Detector<a class="headerlink" href="#retinanet-detector" title="Permalink to this headline">¶</a></h2>
<p>As shown in the figure below, RetinaNet is a single, unified network composed of a backbone network and two task-specific subnetworks.</p>
<a class="reference internal image-reference" href="../_images/retina-net-2.png"><img alt="../_images/retina-net-2.png" src="../_images/retina-net-2.png" style="width: 520pt;" /></a>
<ul class="simple">
<li><p><strong>Feature Pyramid Network Backbone:</strong> The authors adopt the FPN from <em>Feature Pyramid Networks for Object Detection</em> as the backbone network for RetinaNet.</p></li>
<li><p><strong>Anchors:</strong> The authors use translation-invariant anchor boxes similar to those in the RPN variant in <em>Feature Pyramid Networks for Object Detection</em>.</p></li>
<li><p><strong>Classification Subnet:</strong> The classification subnet predicts the probability of object presence at each spatial position for each of anchor and object class. This subnet is a small FCN attached to each FPN level; parameters of this subnet are shared across all pyramid levels.</p></li>
<li><p><strong>Box Regression Subnet:</strong> In parallel with the object classification subnet, another small FCN is attached to each pyramid level for the purpose of regressing the offset from each anchor box to a nearby ground-truth object.</p></li>
<li><p><strong>Initialization:</strong> For the final conv layer of the classification subnet, the bias initialization is set to <span class="math notranslate nohighlight">\(b = - \log((1 - \pi) / \pi)\)</span>, where <span class="math notranslate nohighlight">\(\pi\)</span> specifies that at the start of training every anchor should be labeled as foreground with confidence of <span class="math notranslate nohighlight">\(\sim \pi\)</span>. This initialization prevents the large number of background anchors from generating a large, destabilizing loss value in the first iteration of training.</p></li>
</ul>
</div>
<div class="section" id="results">
<h2>Results<a class="headerlink" href="#results" title="Permalink to this headline">¶</a></h2>
<p><strong>OHEM:</strong> OHEM is another approach to put more emphasis on misclassified examples. But unlike FL, OHEM completely discards easy examples. Experiments show that FL achieve better results (for around 3.2 AP) than OHEM.</p>
</div>
</div>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="../index.html">Table of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Focal Loss for Dense Object Detection (RetinaNet)</a><ul>
<li><a class="reference internal" href="#focal-loss">Focal Loss</a></li>
<li><a class="reference internal" href="#retinanet-detector">RetinaNet Detector</a></li>
<li><a class="reference internal" href="#results">Results</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="../paa.html"
                        title="previous chapter">Probabilistic Anchor Assignment with IoU Prediction for Object Detection</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="detr.html"
                        title="next chapter">End-to-End Object Detection with Transformers (DETR)</a></p>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../_sources/paper-reading/retina-net.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="detr.html" title="End-to-End Object Detection with Transformers (DETR)"
             >next</a> |</li>
        <li class="right" >
          <a href="../paa.html" title="Probabilistic Anchor Assignment with IoU Prediction for Object Detection"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">Mofii Notes 1.0.1 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../paper-reading.html" >Paper Reading Notes</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="../obj-det.html" >Object Detection</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Focal Loss for Dense Object Detection (RetinaNet)</a></li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2020, Mofii.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 3.1.2.
    </div>
  </body>
</html>