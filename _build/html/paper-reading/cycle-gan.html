
<!DOCTYPE html>

<html lang="cn">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks &#8212; Mofii Notes 1.0.1 documentation</title>
    <link rel="stylesheet" href="../_static/classic.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Image-to-Image Translation with Conditional Adversarial Networks (Pix2Pix)" href="pix2pix.html" />
    <link rel="prev" title="Improved Techniques for Training GANs" href="improved-tech-gans.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="pix2pix.html" title="Image-to-Image Translation with Conditional Adversarial Networks (Pix2Pix)"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="improved-tech-gans.html" title="Improved Techniques for Training GANs"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">Mofii Notes 1.0.1 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../paper-reading.html" >Paper Reading Notes</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="../generative.html" accesskey="U">Generative Models</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks</a></li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="unpaired-image-to-image-translation-using-cycle-consistent-adversarial-networks">
<h1>Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks<a class="headerlink" href="#unpaired-image-to-image-translation-using-cycle-consistent-adversarial-networks" title="Permalink to this headline">¶</a></h1>
<div class="line-block">
<div class="line"><strong>Year:</strong> Mar 2017</div>
<div class="line"><strong>Authors:</strong> Jun-Yan Zhu, Taesung Park, Phillip Isola, Alexei A. Efros</div>
<div class="line"><strong>Affiliations:</strong> Berkeley AI Research Laboratory</div>
</div>
<p>In this work, the authors present an approach for learning to translate an image from a source domain <span class="math notranslate nohighlight">\(X\)</span> to a target domain <span class="math notranslate nohighlight">\(Y\)</span> in the absence of paired examples <span class="math notranslate nohighlight">\(\{x_i, y_i\}_{i=1}^N\)</span>. The goal is to learn a mapping <span class="math notranslate nohighlight">\(G: X \to Y\)</span> such that <span class="math notranslate nohighlight">\(G(X)\)</span> is indistinguishable from <span class="math notranslate nohighlight">\(Y\)</span>. Because this mapping is highly under-constrained, they couple it with an inverse mapping <span class="math notranslate nohighlight">\(F: Y \to X\)</span> and introduce a cycle consistency loss to enforce <span class="math notranslate nohighlight">\(F(G(X)) \approx X\)</span>, and vice versa.</p>
<div class="section" id="adversarial-loss">
<h2>Adversarial Loss<a class="headerlink" href="#adversarial-loss" title="Permalink to this headline">¶</a></h2>
<p>For the mapping function <span class="math notranslate nohighlight">\(G: X \to Y\)</span> and its discriminator <span class="math notranslate nohighlight">\(D_Y\)</span>, the objective is expressed as:</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}_\text{GAN}(G, D_Y, X, Y) = \mathbb{E}_{y \sim p_\text{data}(y)}[\log D_Y(y)] + \mathbb{E}_{x \sim p_\text{data}(x)}[\log (1 - D_Y(G(x)))]\]</div>
<p>A similar adversarial loss is applied for the mapping function <span class="math notranslate nohighlight">\(F: Y \to X\)</span>.</p>
</div>
<div class="section" id="cycle-consistency-loss">
<h2>Cycle Consistency Loss<a class="headerlink" href="#cycle-consistency-loss" title="Permalink to this headline">¶</a></h2>
<p>Adversarial losses alone cannot guarantee that the learned function can map an individual input <span class="math notranslate nohighlight">\(x_i\)</span> to a desired output <span class="math notranslate nohighlight">\(y_i\)</span>. To further reduce the space of possible mapping functions, the authors argue that the learned mapping functions should be cycle-consistent: for each image <span class="math notranslate nohighlight">\(x\)</span> from domain <span class="math notranslate nohighlight">\(X\)</span>, the image translation cycle should be able to bring <span class="math notranslate nohighlight">\(x\)</span> back to the original image, i.e., <span class="math notranslate nohighlight">\(x \to G(x) \to F(G(x)) \approx x\)</span>. This is called <strong>forward cycle consistency</strong>, and we have <strong>backward cycle consistency</strong>: <span class="math notranslate nohighlight">\(y \to F(y) \to G(F(y)) \approx y\)</span>. The <strong>cycle consistency loss</strong> is given by</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}_\text{cyc}(G, F) = \mathbb{E}_{x \sim p_\text{data}(x)} [\lVert F(G(x)) - x\rVert_1] + \mathbb{E}_{y \sim p_\text{data}(y)}[\lVert G(F(y)) - y\rVert_1]\]</div>
<p>The full objective is then:</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(G, F, D_X, D_Y) = \mathcal{L}_\text{GAN}(G, D_Y, X, Y) + \mathcal{L}_\text{GAN}(F, D_Y, Y, X) + \lambda \mathcal{L}_\text{cyc}(G, F)\]</div>
<a class="reference internal image-reference" href="../_images/cycle-gan-1.png"><img alt="../_images/cycle-gan-1.png" src="../_images/cycle-gan-1.png" style="width: 480pt;" /></a>
</div>
<div class="section" id="experiment-results">
<h2>Experiment Results<a class="headerlink" href="#experiment-results" title="Permalink to this headline">¶</a></h2>
<p>During training, the authors follow the strategy in ** and update the discriminator using a history of generated images rather than the ones produced by the latest generators.</p>
<a class="reference internal image-reference" href="../_images/cycle-gan-2.png"><img alt="../_images/cycle-gan-2.png" src="../_images/cycle-gan-2.png" style="width: 360pt;" /></a>
<a class="reference internal image-reference" href="../_images/cycle-gan-3.png"><img alt="../_images/cycle-gan-3.png" src="../_images/cycle-gan-3.png" style="width: 560pt;" /></a>
<img alt="../_images/cycle-gan-4.png" src="../_images/cycle-gan-4.png" />
</div>
<div class="section" id="thoughts">
<h2>Thoughts<a class="headerlink" href="#thoughts" title="Permalink to this headline">¶</a></h2>
<p>This architecture is tailored for good performance on the appearance changes, with little success on geometric changes.</p>
</div>
</div>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="../index.html">Table of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks</a><ul>
<li><a class="reference internal" href="#adversarial-loss">Adversarial Loss</a></li>
<li><a class="reference internal" href="#cycle-consistency-loss">Cycle Consistency Loss</a></li>
<li><a class="reference internal" href="#experiment-results">Experiment Results</a></li>
<li><a class="reference internal" href="#thoughts">Thoughts</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="improved-tech-gans.html"
                        title="previous chapter">Improved Techniques for Training GANs</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="pix2pix.html"
                        title="next chapter">Image-to-Image Translation with Conditional Adversarial Networks (Pix2Pix)</a></p>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../_sources/paper-reading/cycle-gan.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="pix2pix.html" title="Image-to-Image Translation with Conditional Adversarial Networks (Pix2Pix)"
             >next</a> |</li>
        <li class="right" >
          <a href="improved-tech-gans.html" title="Improved Techniques for Training GANs"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">Mofii Notes 1.0.1 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../paper-reading.html" >Paper Reading Notes</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="../generative.html" >Generative Models</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks</a></li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2020, Mofii.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 3.1.2.
    </div>
  </body>
</html>