
<!DOCTYPE html>

<html lang="cn">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Improving Deep Video Compression by Resolution-Adaptive Flow Coding &#8212; Mofii Notes 1.0.1 documentation</title>
    <link rel="stylesheet" href="../_static/classic.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Other Papers" href="../other-papers.html" />
    <link rel="prev" title="An End-to-End Learning Framework for Video Compression" href="e2e-learning-framework-vid-comp.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../other-papers.html" title="Other Papers"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="e2e-learning-framework-vid-comp.html" title="An End-to-End Learning Framework for Video Compression"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">Mofii Notes 1.0.1 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../paper-reading.html" >Paper Reading Notes</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="../compression-vid.html" accesskey="U">Compression (Video)</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Improving Deep Video Compression by Resolution-Adaptive Flow Coding</a></li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="improving-deep-video-compression-by-resolution-adaptive-flow-coding">
<h1>Improving Deep Video Compression by Resolution-Adaptive Flow Coding<a class="headerlink" href="#improving-deep-video-compression-by-resolution-adaptive-flow-coding" title="Permalink to this headline">¶</a></h1>
<div class="line-block">
<div class="line"><strong>Year:</strong> Sep 2020</div>
<div class="line"><strong>Authors:</strong> Zhihao Hu, Zhenghao Chen, Dong Xu, Guo Lu, Wanli Ouyang, Shuhang Gu</div>
<div class="line"><strong>Affiliations:</strong> Beihang University, The University of Sydney, Beijing Institute of Technology</div>
</div>
<p>Previous video compression frameworks adopt single representations for both input flow maps and output motion features using a single motion vector (MV) encoder. The authors argue that this cannot effectively handle complex or simple motion patterns in different scenes and fast or slow movement of objects.</p>
<p>In this work, the authors propose a new framework called Resolution-adaptive Flow Coding (RaFC), which can adopt multi-resolution representations for both flow maps and motion features and then automatically decide the optimal resolutions at both frame-level and block-level in order to achieve the optimal rate-distortion trade-off.</p>
<p>Experiment results on four benchmark datasets, HEVC, VTL, UVG, and MCL-JCV, demonstrates the overall RaFC framework outperforms the baseline algorithms including H.264, H.265, and DVC.</p>
<div class="section" id="system-overview">
<h2>System Overview<a class="headerlink" href="#system-overview" title="Permalink to this headline">¶</a></h2>
<p>The overall coding procedure is summarized in the following steps:</p>
<dl class="simple">
<dt><strong>Motion coding.</strong> The RaFC method is utilized for motion coding. RaFC consists of three modules, motion estimation net, the motion vector (MV) encoder net, and the MV decoder net.</dt><dd><ul class="simple">
<li><p>The <strong>motion estimation net</strong> estimates the optical flow <span class="math notranslate nohighlight">\(V_t\)</span> between the input frame <span class="math notranslate nohighlight">\(X_t\)</span> and the previous reconstructed frame <span class="math notranslate nohighlight">\(\hat{X}_{t-1}\)</span>.</p></li>
<li><p>The <strong>MV encoder</strong> encodes the optical flow maps as motion feautres/representations <span class="math notranslate nohighlight">\(M_t\)</span>, which is further quantized as <span class="math notranslate nohighlight">\(\hat{M}_t\)</span> before entropy coding.</p></li>
<li><p>The <strong>MV decoder</strong> decodes the motion representation <span class="math notranslate nohighlight">\(\hat{M}_t\)</span> so that the reconstructed flow map <span class="math notranslate nohighlight">\(\hat{V}_t\)</span> is obtained.</p></li>
</ul>
</dd>
</dl>
<p><strong>Motion copmensation.</strong> Based on <span class="math notranslate nohighlight">\(\hat{V}_t\)</span> and <span class="math notranslate nohighlight">\(\hat{X}_{t-1}\)</span>, a motion compensation network predicts <span class="math notranslate nohighlight">\(\bar{X}_t\)</span>.</p>
<p><strong>Residual coding.</strong> Let <span class="math notranslate nohighlight">\(R_t\)</span> be the residual between <span class="math notranslate nohighlight">\(X_t\)</span> and <span class="math notranslate nohighlight">\(\bar{X}_t\)</span>. A residual encoder encodes the <span class="math notranslate nohighlight">\(R_t\)</span> as <span class="math notranslate nohighlight">\(Y_t\)</span> and then quantized as <span class="math notranslate nohighlight">\(\hat{Y}_t\)</span> for entropy coding. A residual decoder network reconstructs <span class="math notranslate nohighlight">\(\hat{R}_t\)</span> from <span class="math notranslate nohighlight">\(\hat{Y}_t\)</span>.</p>
<p><strong>Frame reconstruction.</strong> The final reconstructed frame is obtained by <span class="math notranslate nohighlight">\(\hat{X}_t = \bar{X}_t + \hat{R}_t\)</span>.</p>
<p><strong>Quantization and Bit Estimation.</strong> The authors follow the method in [1] and add uniform noise to approximate quantization in the training stage. They use the bitrate estimation network in [1] to estimate the entropy encoding bits.</p>
<a class="reference internal image-reference" href="../_images/improv-dvc-by-res-adap-flow-coding-1.png"><img alt="../_images/improv-dvc-by-res-adap-flow-coding-1.png" src="../_images/improv-dvc-by-res-adap-flow-coding-1.png" style="width: 400pt;" /></a>
</div>
<div class="section" id="problem-formulation">
<h2>Problem Formulation<a class="headerlink" href="#problem-formulation" title="Permalink to this headline">¶</a></h2>
<p>Let <span class="math notranslate nohighlight">\(X = \{X_1, \dots, X_{t-1}, X_t, \dots\}\)</span> be the input video sequence to be compressed, where <span class="math notranslate nohighlight">\(X_t \in \mathbb{R}^{W \times H \times C}\)</span>. The video encoder will generate the corresponding bitstreams, while the deocder reconstructs the video sequences using the received bitstreams. The objective of the learning based video compression system is formulated as follows,</p>
<div class="math notranslate nohighlight">
\[RD = R + \lambda D = (\mathbb{H}(\hat{M}_t) + \mathbb{H}(\hat{Y}_t)) + \lambda d(X_t, \hat{X}_t)\]</div>
<p>where <span class="math notranslate nohighlight">\(d(\cdot)\)</span> represents the metric, MSE or MS-SSIM. The <strong>rate-distortion optimization</strong> (RDO) procedure is summarized as follows,</p>
<div class="math notranslate nohighlight">
\[\mathcal{M} = \text{argmin}_{i \in \mathcal{C}} RD_i\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathcal{C}\)</span> represents the candidate modes for coding block. In the RaFC framework, the key idea is to use the RDO technique to select the optimal resolution of optical flow maps or motion features at each block for the current frame.</p>
</div>
<div class="section" id="resolution-adaptive-flow-coding-rafc">
<h2>Resolution-Adaptive Flow Coding (RaFC)<a class="headerlink" href="#resolution-adaptive-flow-coding-rafc" title="Permalink to this headline">¶</a></h2>
<div class="section" id="a-frame-level-scheme-rafc-frame">
<h3>(a) Frame-Level Scheme RaFC-Frame<a class="headerlink" href="#a-frame-level-scheme-rafc-frame" title="Permalink to this headline">¶</a></h3>
<p>Given <span class="math notranslate nohighlight">\(X_t\)</span> and <span class="math notranslate nohighlight">\(\hat{X}_{t-1}\)</span>, the motion estimation network is utilized to generate multi-scale flow maps. Taking advantage of the existing pyramid architecture in Spynet [2], the authors generate two flow maps <span class="math notranslate nohighlight">\(V_t^1\)</span> and <span class="math notranslate nohighlight">\(V_t^2\)</span> with resolutions <span class="math notranslate nohighlight">\(W \times H\)</span> and <span class="math notranslate nohighlight">\(W/2 \times H/2\)</span>.</p>
<p>By calculating the rate-distortion (RD) value for <span class="math notranslate nohighlight">\(V_t^1\)</span> and <span class="math notranslate nohighlight">\(V_t^2\)</span>, the optimal flow map with the minimum RD value is selected.</p>
</div>
<div class="section" id="b-block-level-scheme-rafc-block">
<h3>(b) Block-Level Scheme RaFC-Block<a class="headerlink" href="#b-block-level-scheme-rafc-block" title="Permalink to this headline">¶</a></h3>
<dl class="simple">
<dt>It is necessary to design an efficient multi-scale motion features to handle different types of motion patterns. As shown in the figure below, given the optical flow map <span class="math notranslate nohighlight">\(V_t\)</span> (<span class="math notranslate nohighlight">\(V_t^1\)</span> or <span class="math notranslate nohighlight">\(V_t^2\)</span>), we first feed the optical flow map <span class="math notranslate nohighlight">\(V_t\)</span> to generate the multi-scale motion features <span class="math notranslate nohighlight">\(m_t^1\)</span> and <span class="math notranslate nohighlight">\(m_t^2\)</span>. Then the proposed RaFC-block method will select the optimal resolutino of the motion features for each block based on the RDO technique:</dt><dd><ul class="simple">
<li><p>Indicator map geneartion</p></li>
<li><p>Motion feature reorganization</p></li>
</ul>
</dd>
</dl>
<a class="reference internal image-reference" href="../_images/improv-dvc-by-res-adap-flow-coding-2.png"><img alt="../_images/improv-dvc-by-res-adap-flow-coding-2.png" src="../_images/improv-dvc-by-res-adap-flow-coding-2.png" style="width: 320pt;" /></a>
<a class="reference internal image-reference" href="../_images/improv-dvc-by-res-adap-flow-coding-3.png"><img alt="../_images/improv-dvc-by-res-adap-flow-coding-3.png" src="../_images/improv-dvc-by-res-adap-flow-coding-3.png" style="width: 320pt;" /></a>
</div>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<p><strong>[1]</strong> Ballé, J., Minnen, D., Singh, S., Hwang, S. J., &amp; Johnston, N. (2018). Variational image compression with a scale hyperprior. arXiv preprint arXiv:1802.01436.</p>
<p><strong>[2]</strong> Ranjan, A., &amp; Black, M. J. (2017). Optical flow estimation using a spatial pyramid network. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 4161-4170).</p>
</div>
</div>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="../index.html">Table of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Improving Deep Video Compression by Resolution-Adaptive Flow Coding</a><ul>
<li><a class="reference internal" href="#system-overview">System Overview</a></li>
<li><a class="reference internal" href="#problem-formulation">Problem Formulation</a></li>
<li><a class="reference internal" href="#resolution-adaptive-flow-coding-rafc">Resolution-Adaptive Flow Coding (RaFC)</a><ul>
<li><a class="reference internal" href="#a-frame-level-scheme-rafc-frame">(a) Frame-Level Scheme RaFC-Frame</a></li>
<li><a class="reference internal" href="#b-block-level-scheme-rafc-block">(b) Block-Level Scheme RaFC-Block</a></li>
</ul>
</li>
<li><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="e2e-learning-framework-vid-comp.html"
                        title="previous chapter">An End-to-End Learning Framework for Video Compression</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="../other-papers.html"
                        title="next chapter">Other Papers</a></p>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../_sources/paper-reading/improv-dvc-by-res-adap-flow-coding.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../other-papers.html" title="Other Papers"
             >next</a> |</li>
        <li class="right" >
          <a href="e2e-learning-framework-vid-comp.html" title="An End-to-End Learning Framework for Video Compression"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">Mofii Notes 1.0.1 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../paper-reading.html" >Paper Reading Notes</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="../compression-vid.html" >Compression (Video)</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Improving Deep Video Compression by Resolution-Adaptive Flow Coding</a></li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2020, Mofii.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 3.1.2.
    </div>
  </body>
</html>