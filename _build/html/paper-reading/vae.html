
<!DOCTYPE html>

<html lang="cn">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Auto-Encoding Variational Bayes &#8212; Mofii Notes 1.0.1 documentation</title>
    <link rel="stylesheet" href="../_static/classic.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets" href="../infogan.html" />
    <link rel="prev" title="Generative Models" href="../generative.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../infogan.html" title="InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="../generative.html" title="Generative Models"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">Mofii Notes 1.0.1 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../paper-reading.html" >Paper Reading Notes</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="../generative.html" accesskey="U">Generative Models</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Auto-Encoding Variational Bayes</a></li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="auto-encoding-variational-bayes">
<h1>Auto-Encoding Variational Bayes<a class="headerlink" href="#auto-encoding-variational-bayes" title="Permalink to this headline">¶</a></h1>
<div class="section" id="tutorial-on-variational-autoencoders">
<h2>Tutorial on Variational Autoencoders<a class="headerlink" href="#tutorial-on-variational-autoencoders" title="Permalink to this headline">¶</a></h2>
<div class="line-block">
<div class="line"><strong>Authors:</strong> Carl Doersch</div>
<div class="line"><strong>Affiliations:</strong> Carnegie Mellon, UC Berkeley</div>
</div>
<dl class="simple">
<dt>Training generative models has been a long-standing problem and there are three main drawbacks:</dt><dd><ol class="arabic simple">
<li><p>they might require strong assumptions about the structure of the data</p></li>
<li><p>they might make severe approximations, leading to suboptimal models</p></li>
<li><p>they might rely on computationally expensive inference procedures like Markov Chain Monte Carlo</p></li>
</ol>
</dd>
</dl>
<hr class="docutils" />
<div class="line-block">
<div class="line"><strong>Authors:</strong> Diederik P. Kingma, Max Welling</div>
<div class="line"><strong>Affiliations:</strong> Universiteit van Amsterdam</div>
</div>
<p>In this work, the authors propose a stochastic variational inference and learning algorithm that performs efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions and large datasets.</p>
<dl class="simple">
<dt>Their contributions is two-fold:</dt><dd><ol class="arabic simple">
<li><p>They show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods.</p></li>
<li><p>They show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model to the intractable posterior using the proposed lower bound estimator.</p></li>
</ol>
</dd>
</dl>
<p>When a neural network is used for the recognition model, we arrive at the <strong>variational auto-encoder</strong>.</p>
</div>
<div class="section" id="method">
<h2>Method<a class="headerlink" href="#method" title="Permalink to this headline">¶</a></h2>
<p>The strategy in this section can be used to derive a lower bound estimator (a stochastic objective function) for a variety of directed graphical models with continuous latent variables. We will restrict ourselves here to the common case where we have an i.i.d. dataset with latent variables per datapoint, and where we like to perform maximum likelihood (ML) or maximum a posteriori (MAP) inference on the (global) parameters, and variational inference on the latent variables.</p>
<div class="section" id="problem-scenario">
<h3>Problem Scenario<a class="headerlink" href="#problem-scenario" title="Permalink to this headline">¶</a></h3>
<dl class="simple">
<dt>Let us consider some dataset <span class="math notranslate nohighlight">\(\mathbf{X} = \{\mathbf{x}^{(i)}\}_{i=1}^N\)</span> consisting of <span class="math notranslate nohighlight">\(N\)</span> i.i.d. samples. We assume that the data are generated by some random process, involving an unobserved continuous random variable <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>. This process consists of two steps:</dt><dd><ol class="arabic simple">
<li><p>a value <span class="math notranslate nohighlight">\(\mathbf{z}^{(i)}\)</span> is generated from some prior distribution <span class="math notranslate nohighlight">\(p_{\mathbf{\theta}^*}(\mathbf{z})\)</span></p></li>
<li><p>a value <span class="math notranslate nohighlight">\(\mathbf{x}^{(i)}\)</span> is generated from some conditional distribution <span class="math notranslate nohighlight">\(p_{\mathbf{\theta}^*}(\mathbf{x} \mid \mathbf{z})\)</span></p></li>
</ol>
</dd>
</dl>
<p>We assume that the prior <span class="math notranslate nohighlight">\(p_{\mathbf{\theta}^*}(\mathbf{z})\)</span> and likelihood <span class="math notranslate nohighlight">\(p_{\mathbf{\theta}^*}(\mathbf{x} \mid \mathbf{z})\)</span> come from parametric families of distributions <span class="math notranslate nohighlight">\(p_\mathbf{\theta}(\mathbf{z})\)</span> and <span class="math notranslate nohighlight">\(p_\mathbf{\theta}(\mathbf{x} \mid \mathbf{z})\)</span>, and their PDFs are differentiable almost everywhere w.r.t. both <span class="math notranslate nohighlight">\(\mathbf{\theta}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>. This process is hidden and values of <span class="math notranslate nohighlight">\(\mathbf{\theta}^*\)</span> and <span class="math notranslate nohighlight">\(\mathbf{z}^{(i)}\)</span> are unknown to us.</p>
<dl class="simple">
<dt>The authors do not make common simplifying assumptions about the marginal or posterior probabilities. Conversely, they are interested in a general algorithm that even works efficiently in the case of:</dt><dd><ol class="arabic simple">
<li><p><strong>Intractability:</strong> the case where the integral of the marginal likelihood <span class="math notranslate nohighlight">\(p_\mathbf{\theta}(\mathbf{x}) = \int p_\mathbf{\theta}(\mathbf{z})p_\mathbf{\theta}(\mathbf{x} \mid \mathbf{z})d\mathbf{z}\)</span> is intractable</p></li>
<li><p><strong>A large dataset:</strong> we have so much data that batch optimization is too costly</p></li>
</ol>
</dd>
</dl>
</div>
</div>
<div class="section" id="example-variation-auto-encoder">
<h2>Example: Variation Auto-Encoder<a class="headerlink" href="#example-variation-auto-encoder" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="experiments">
<h2>Experiments<a class="headerlink" href="#experiments" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="future-work">
<h2>Future Work<a class="headerlink" href="#future-work" title="Permalink to this headline">¶</a></h2>
</div>
</div>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="../index.html">Table of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Auto-Encoding Variational Bayes</a><ul>
<li><a class="reference internal" href="#tutorial-on-variational-autoencoders">Tutorial on Variational Autoencoders</a></li>
<li><a class="reference internal" href="#method">Method</a><ul>
<li><a class="reference internal" href="#problem-scenario">Problem Scenario</a></li>
</ul>
</li>
<li><a class="reference internal" href="#example-variation-auto-encoder">Example: Variation Auto-Encoder</a></li>
<li><a class="reference internal" href="#experiments">Experiments</a></li>
<li><a class="reference internal" href="#conclusion">Conclusion</a></li>
<li><a class="reference internal" href="#future-work">Future Work</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="../generative.html"
                        title="previous chapter">Generative Models</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="../infogan.html"
                        title="next chapter">InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets</a></p>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../_sources/paper-reading/vae.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../infogan.html" title="InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets"
             >next</a> |</li>
        <li class="right" >
          <a href="../generative.html" title="Generative Models"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">Mofii Notes 1.0.1 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../paper-reading.html" >Paper Reading Notes</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="../generative.html" >Generative Models</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Auto-Encoding Variational Bayes</a></li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2020, Mofii.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 3.1.2.
    </div>
  </body>
</html>