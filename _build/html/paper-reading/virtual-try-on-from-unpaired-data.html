
<!DOCTYPE html>

<html lang="cn">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Image Based Virtual Try-On Network from Unpaired Data &#8212; Mofii Notes 1.0.1 documentation</title>
    <link rel="stylesheet" href="../_static/classic.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Compression (Image)" href="../compression-img.html" />
    <link rel="prev" title="High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs (Pix2PixHD)" href="high-res-img-syth-semantic-manipulation-cgan.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../compression-img.html" title="Compression (Image)"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="high-res-img-syth-semantic-manipulation-cgan.html" title="High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs (Pix2PixHD)"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">Mofii Notes 1.0.1 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../paper-reading.html" >Paper Reading Notes</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="../generative.html" accesskey="U">Generative Models</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Image Based Virtual Try-On Network from Unpaired Data</a></li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="image-based-virtual-try-on-network-from-unpaired-data">
<h1>Image Based Virtual Try-On Network from Unpaired Data<a class="headerlink" href="#image-based-virtual-try-on-network-from-unpaired-data" title="Permalink to this headline">¶</a></h1>
<div class="line-block">
<div class="line"><strong>Authors:</strong> Assaf Neuberger, Eran Borenstein, Bar Hilleli, Eduard Oks, Sharon Alpert</div>
<div class="line"><strong>Affiliations:</strong> Amazon Lab126</div>
</div>
<p>This paper presents a new image-based virtual try-on approach (Outfit-VITON) that helps visualize how a composition of clothing items selected from various reference images form a cohesive outfit on a person in a query image. The authors provide an inexpensive data collection and training process, and introduce an online optimization capability for virtual try-on that accurately synthesizes fine garment features.</p>
<a class="reference internal image-reference" href="../_images/virtual-try-on-from-unpaired-data-1.png"><img alt="../_images/virtual-try-on-from-unpaired-data-1.png" src="../_images/virtual-try-on-from-unpaired-data-1.png" style="width: 320pt;" /></a>
<div class="section" id="outfit-virtual-try-on-o-viton">
<h2>Outfit Virtual Try-On (O-VITON)<a class="headerlink" href="#outfit-virtual-try-on-o-viton" title="Permalink to this headline">¶</a></h2>
<p>This system uses multiple reference images of people wearing garments varying in shape and style.</p>
<p>Similar to the Pix2PixHD approach, the generator <span class="math notranslate nohighlight">\(G\)</span> is conditioned on a semantic segmentation map and on an appearance map generated by an encoder <span class="math notranslate nohighlight">\(E\)</span>. The autoencoder assigns to each semantic region in the segmentation map a low-dimensional feature vector representing the region appearance. These appearance-based features enable control over the appearance of the output image and address the lack of diversity.</p>
<dl class="simple">
<dt>O-VITON consists of three main steps:</dt><dd><ol class="arabic simple">
<li><p>Generating a segmentation map that consistently combines the sihouttes (shape) of the selected reference garments with the segmentation map of the query image.</p></li>
<li><p>Generating a photo-realistic image dressed with the garments selected from the reference images.</p></li>
<li><p>Online optimization to refine the appearance of the final output image.</p></li>
</ol>
</dd>
</dl>
<a class="reference internal image-reference" href="../_images/virtual-try-on-from-unpaired-data-2.png"><img alt="../_images/virtual-try-on-from-unpaired-data-2.png" src="../_images/virtual-try-on-from-unpaired-data-2.png" style="width: 520pt;" /></a>
</div>
<div class="section" id="feed-forward-generation">
<h2>Feed-Forward Generation<a class="headerlink" href="#feed-forward-generation" title="Permalink to this headline">¶</a></h2>
<dl class="simple">
<dt>The inputs of the system are:</dt><dd><ul class="simple">
<li><p>a <span class="math notranslate nohighlight">\(H \times W\)</span> query image <span class="math notranslate nohighlight">\(x^0\)</span></p></li>
<li><p>a set of <span class="math notranslate nohighlight">\(M\)</span> <span class="math notranslate nohighlight">\(H \times W\)</span> reference images <span class="math notranslate nohighlight">\((x^1, \dots, x^M)\)</span></p></li>
</ul>
</dd>
</dl>
<p>A PSP semantic segmentation network <span class="math notranslate nohighlight">\(S\)</span> is trained which outputs <span class="math notranslate nohighlight">\(s^m = S(x^m)\)</span> of size <span class="math notranslate nohighlight">\(H \times W \times D_c\)</span>. Here class <span class="math notranslate nohighlight">\(c\)</span> can be a body part such as face or right arm, or a garment type such as tops, pants, jacket or background. Further a DensePose network is applied to estimate a body model <span class="math notranslate nohighlight">\(b = B(x^0)\)</span> of size <span class="math notranslate nohighlight">\(H \times W \times D_b\)</span>.</p>
<p><strong>Shape generation:</strong> The shape-generation network combines the body model <span class="math notranslate nohighlight">\(b\)</span> with the shapes of the selected garments <span class="math notranslate nohighlight">\(\{s^m\}_{m=1}^M\)</span>. A shape encoder <span class="math notranslate nohighlight">\(E_{shape}\)</span> followed by a local pooling step maps this mask to a shape feature slice <span class="math notranslate nohighlight">\(e_{m, c}^8 = E_{shape}(M_{m, c})\)</span> of <span class="math notranslate nohighlight">\(8 \times 4 \times D_s\)</span> dimensions. By concatenating them along the depth dimension, we get a coarse shape feature map <span class="math notranslate nohighlight">\(\bar{e}^s\)</span> of <span class="math notranslate nohighlight">\(8 \times 4 \times D_s \times D_c\)</span> dimensions, and the up-scaled version <span class="math notranslate nohighlight">\(e^s\)</span> of <span class="math notranslate nohighlight">\(H \times W \times D_sD_c\)</span> dimensions. The shape feature map <span class="math notranslate nohighlight">\(e^s\)</span> and the body model <span class="math notranslate nohighlight">\(b\)</span> are fed into the shape generator network <span class="math notranslate nohighlight">\(G_{shape}\)</span> to generate a new, transformed segmentation map <span class="math notranslate nohighlight">\(s^y\)</span> of the query person wearing the selected reference garments <span class="math notranslate nohighlight">\(s^y = G_{shape}(b, e^s)\)</span>.</p>
<p><strong>Appearance generation:</strong> An appearance autoencoder <span class="math notranslate nohighlight">\(E_{app}(x^m, s^m)\)</span> is applied to the RGB images and their segmentation maps <span class="math notranslate nohighlight">\((x^m, s^m)\)</span>. The output is denoted as <span class="math notranslate nohighlight">\(\bar{e}_m^t\)</span> of <span class="math notranslate nohighlight">\(H \times W \times D_t\)</span> dimensions. By region-wise average pooling according to the mask <span class="math notranslate nohighlight">\(M_{m, c}\)</span> they form a <span class="math notranslate nohighlight">\(D_t\)</span> dimensional vector <span class="math notranslate nohighlight">\(e_{m, c}^t\)</span>. The appearance generator <span class="math notranslate nohighlight">\(G_{app}\)</span> takes the segmentation map <span class="math notranslate nohighlight">\(s^y\)</span> and the appearance featuer map <span class="math notranslate nohighlight">\(e^t\)</span> as the condition and generates an output <span class="math notranslate nohighlight">\(y\)</span> representing the feed-forward virtual try-on output <span class="math notranslate nohighlight">\(y = G_{app}(s^y, e^t)\)</span>.</p>
</div>
<div class="section" id="train-phase">
<h2>Train Phase<a class="headerlink" href="#train-phase" title="Permalink to this headline">¶</a></h2>
<a class="reference internal image-reference" href="../_images/virtual-try-on-from-unpaired-data-3.png"><img alt="../_images/virtual-try-on-from-unpaired-data-3.png" src="../_images/virtual-try-on-from-unpaired-data-3.png" style="width: 400pt;" /></a>
</div>
<div class="section" id="online-optimization">
<h2>Online Optimization<a class="headerlink" href="#online-optimization" title="Permalink to this headline">¶</a></h2>
<dl class="simple">
<dt>There are two remaining issues:</dt><dd><ol class="arabic simple">
<li><p>Less frequent garments with non-repetitive patterns are more challenging due to both their irregular pattern and reduced representation.</p></li>
<li><p>No matter how big the training set is, it will never be sufficiently large to cover all possible garment pattern and shape variations.</p></li>
</ol>
</dd>
</dl>
</div>
<div class="section" id="thoughts">
<h2>Thoughts<a class="headerlink" href="#thoughts" title="Permalink to this headline">¶</a></h2>
<div class="section" id="problem">
<h3>Problem<a class="headerlink" href="#problem" title="Permalink to this headline">¶</a></h3>
<p>Image-based virtual try-on that supports synthesizing a single, coherent outfit that consists of multiple garments:</p>
<a class="reference internal image-reference" href="../_images/virtual-try-on-from-unpaired-data-4.png"><img alt="../_images/virtual-try-on-from-unpaired-data-4.png" src="../_images/virtual-try-on-from-unpaired-data-4.png" style="width: 320pt;" /></a>
</div>
<div class="section" id="main-contributions">
<h3>Main contributions<a class="headerlink" href="#main-contributions" title="Permalink to this headline">¶</a></h3>
<ol class="arabic simple">
<li><p>A new virtual try-on framework, Outfit-VITON</p></li>
<li><p>An inexpensive data collection and training process that uses unpaired data</p></li>
<li><p>An online optimization for fine garment features like textures, logos and embroidery</p></li>
</ol>
</div>
<div class="section" id="method-overview">
<h3>Method Overview<a class="headerlink" href="#method-overview" title="Permalink to this headline">¶</a></h3>
<ol class="arabic simple">
<li><p><strong>Shape generation:</strong> generating a segmentation map that combines the sihouttes of the garments with the segmentation map of the query image
- A PSP semantic segmentation model is used to segment the query and reference images into <span class="math notranslate nohighlight">\(D_c\)</span> classes which include body parts and garment types
- A DensePose network is used to capture the pose and body shape of humans <span class="math notranslate nohighlight">\(b\)</span>
- A shape autoencoder encodes the one-hot segmentation of <span class="math notranslate nohighlight">\(H \times W \times D_c\)</span> into <span class="math notranslate nohighlight">\(8 \times 4 \times D_s\)</span> dimensions
- The shape generator is given by <span class="math notranslate nohighlight">\(s^y = G_{shape}(b, e^s)\)</span></p></li>
<li><p><strong>Appearance generation:</strong> generating a photo-realistic image showing the person in the query image dressed with the selected garments
- An appearance encoder that encode appearance features into <span class="math notranslate nohighlight">\(H \times W \times D_t\)</span> and then pooling to <span class="math notranslate nohighlight">\(1 \times D_t\)</span>
- The appearance generator is given by <span class="math notranslate nohighlight">\(y = G_{app}(s^y, e^t)\)</span></p></li>
<li><p><strong>Online optimization:</strong> refine the appearance</p></li>
</ol>
</div>
<div class="section" id="id1">
<h3>Train Phase<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<a class="reference internal image-reference" href="../_images/virtual-try-on-from-unpaired-data-3.png"><img alt="../_images/virtual-try-on-from-unpaired-data-3.png" src="../_images/virtual-try-on-from-unpaired-data-3.png" style="width: 400pt;" /></a>
</div>
<div class="section" id="idea">
<h3>Idea<a class="headerlink" href="#idea" title="Permalink to this headline">¶</a></h3>
<p>What about using the framework of a CycleGAN? Consider the case below:</p>
<a class="reference internal image-reference" href="paper-reading/figures/virtual-try-on-from-unpaired-data-6.png"><img alt="paper-reading/figures/virtual-try-on-from-unpaired-data-6.png" src="paper-reading/figures/virtual-try-on-from-unpaired-data-6.png" style="width: 400pt;" /></a>
<p>Let a training pair be <span class="math notranslate nohighlight">\(x^0 + (x^1, x^2)\)</span> where <span class="math notranslate nohighlight">\(x^0\)</span> is the query image and <span class="math notranslate nohighlight">\(x^1\)</span>, <span class="math notranslate nohighlight">\(x^2\)</span> are two reference images. The output of the framework would be:</p>
<div class="math notranslate nohighlight">
\[y = G_{app}(G_{shape}(x^0, (x^1, x^2)))\]</div>
<p>We should further enforce that</p>
<div class="math notranslate nohighlight">
\[\begin{split}x^1 &amp; = G_{app}(G_{shape}(x^0, (y, x^2))) \\
x^2 &amp; = G_{app}(G_{shape}(x^0, (x^1, y)))\end{split}\]</div>
</div>
</div>
</div>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="../index.html">Table of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Image Based Virtual Try-On Network from Unpaired Data</a><ul>
<li><a class="reference internal" href="#outfit-virtual-try-on-o-viton">Outfit Virtual Try-On (O-VITON)</a></li>
<li><a class="reference internal" href="#feed-forward-generation">Feed-Forward Generation</a></li>
<li><a class="reference internal" href="#train-phase">Train Phase</a></li>
<li><a class="reference internal" href="#online-optimization">Online Optimization</a></li>
<li><a class="reference internal" href="#thoughts">Thoughts</a><ul>
<li><a class="reference internal" href="#problem">Problem</a></li>
<li><a class="reference internal" href="#main-contributions">Main contributions</a></li>
<li><a class="reference internal" href="#method-overview">Method Overview</a></li>
<li><a class="reference internal" href="#id1">Train Phase</a></li>
<li><a class="reference internal" href="#idea">Idea</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="high-res-img-syth-semantic-manipulation-cgan.html"
                        title="previous chapter">High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs (Pix2PixHD)</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="../compression-img.html"
                        title="next chapter">Compression (Image)</a></p>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../_sources/paper-reading/virtual-try-on-from-unpaired-data.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../compression-img.html" title="Compression (Image)"
             >next</a> |</li>
        <li class="right" >
          <a href="high-res-img-syth-semantic-manipulation-cgan.html" title="High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs (Pix2PixHD)"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">Mofii Notes 1.0.1 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../paper-reading.html" >Paper Reading Notes</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="../generative.html" >Generative Models</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Image Based Virtual Try-On Network from Unpaired Data</a></li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2020, Mofii.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 3.1.2.
    </div>
  </body>
</html>