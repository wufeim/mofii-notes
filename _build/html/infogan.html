
<!DOCTYPE html>

<html lang="cn">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets &#8212; Mofii Notes 1.0.1 documentation</title>
    <link rel="stylesheet" href="_static/classic.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/language_data.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Improved Techniques for Training GANs" href="paper-reading/improved-tech-gans.html" />
    <link rel="prev" title="Auto-Encoding Variational Bayes" href="paper-reading/vae.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="paper-reading/improved-tech-gans.html" title="Improved Techniques for Training GANs"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="paper-reading/vae.html" title="Auto-Encoding Variational Bayes"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">Mofii Notes 1.0.1 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="paper-reading.html" >Paper Reading Notes</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="generative.html" accesskey="U">Generative Models</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets</a></li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="infogan-interpretable-representation-learning-by-information-maximizing-generative-adversarial-nets">
<h1>InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets<a class="headerlink" href="#infogan-interpretable-representation-learning-by-information-maximizing-generative-adversarial-nets" title="Permalink to this headline">¶</a></h1>
<div class="line-block">
<div class="line"><strong>Authors:</strong> Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, Pieter Abbeel</div>
<div class="line"><strong>Affiliations:</strong> OpenAI</div>
</div>
<p>This paper describes InfoGAN, an information-theoretic extension to the GAN that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN maximizes the mutual information between a small subset of the latent variables and the observations.</p>
<div class="section" id="mutual-information-for-inducing-latent-codes">
<h2>Mutual Information for Inducing Latent Codes<a class="headerlink" href="#mutual-information-for-inducing-latent-codes" title="Permalink to this headline">¶</a></h2>
<p>The minimax game defined by GAN is given by</p>
<div class="math notranslate nohighlight">
\[\min_G \max_D V(D, G) = \mathbb{E}_{x \sim P_\text{data}} [\log D(x)] + \mathbb{E}_{z \sim \text{noise}}[\log (1 - D(G(z)))]\]</div>
<p>This GAN formulation imposes no restrictions on the manner in which the generator may use this noise <span class="math notranslate nohighlight">\(z\)</span>, which may lead to a highly entangled representation.</p>
<p>The authors propose to decompose the input noise vector into two parts: (i) <span class="math notranslate nohighlight">\(z\)</span>, which is treated as source of incompressible noise; (ii) <span class="math notranslate nohighlight">\(c\)</span>, which we will call the latent code and will target the salient structured semantic features of the data distribution.</p>
<p>Now the form of the generator becomes <span class="math notranslate nohighlight">\(G(z, c)\)</span>. However, standard GAN may ignore the additional latent code <span class="math notranslate nohighlight">\(c\)</span> by finding a solution satisfying <span class="math notranslate nohighlight">\(P_G(x \mid c) = P_G(x)\)</span>. To cope with this problem of trivial codes, the authors propose an information-theoretic regularization: there should be high mutual information between latent codes <span class="math notranslate nohighlight">\(c\)</span> and generator distribution <span class="math notranslate nohighlight">\(G(z, c)\)</span>.</p>
<p>Given any <span class="math notranslate nohighlight">\(x \sim P_G(x)\)</span>, we want <span class="math notranslate nohighlight">\(P_G(c \mid x)\)</span> to have a small entropy. The information in the latent code <span class="math notranslate nohighlight">\(c\)</span> should not be lost in the generation process. The authors propose to solve the following information-regularized minimax game:</p>
<div class="math notranslate nohighlight">
\[\min_G \max_D V_I(D, G) = V(D, G) - \lambda I(c; G(z, c))\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Mutual Information</strong></p>
<p>In information theory, mutual information between two random variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, <span class="math notranslate nohighlight">\(I(X, Y)\)</span>, measure the &quot;amount of information&quot; learned from knowledge of <span class="math notranslate nohighlight">\(Y\)</span> about <span class="math notranslate nohighlight">\(X\)</span>. The mutal information can be expressed as the difference between two entropy terms:</p>
<div class="math notranslate nohighlight">
\[I(X; Y) = H(X) - H(X \mid Y) = H(Y) - H(Y \mid X)\]</div>
<p>This definition has an intuitive interpretation: <span class="math notranslate nohighlight">\(I(X; Y)\)</span> is the reduction of uncertainty in <span class="math notranslate nohighlight">\(X\)</span> when <span class="math notranslate nohighlight">\(Y\)</span> is observed.</p>
</div>
</div>
<div class="section" id="variational-mutual-information-maximization">
<h2>Variational Mutual Information Maximization<a class="headerlink" href="#variational-mutual-information-maximization" title="Permalink to this headline">¶</a></h2>
<p>The mutual information term <span class="math notranslate nohighlight">\(I(c; G(z, c))\)</span> is hard to maximize directly as it requires access to the posterior <span class="math notranslate nohighlight">\(P(c \mid x)\)</span>. The authors define an auxiliary distribution <span class="math notranslate nohighlight">\(Q(c \mid x)\)</span> to approximate <span class="math notranslate nohighlight">\(P(c \mid x)\)</span> and obtain a lower bound of <span class="math notranslate nohighlight">\(I(c; G(z, c))\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}I(c; G(z, c)) &amp; = H(c) - H(c \mid G(z, c)) \\
&amp; = \mathbb{E}_{x \sim G(z, c)}[\mathbb{E}_{c' \sim P(c \mid x)}[\log P(c' \mid x)]] + H(c) \\
&amp; = \mathbb{E}_{x \sim G(z, c)}[D_{KL}(P(\cdot \mid x) \mid\mid Q(\cdot \mid x)) + \mathbb{E}_{c' \sim P(c \mid x)}[\log Q(c' \mid x)]] + H(c) \\
&amp; \geq \mathbb{E}_{x \sim G(z, c)}[\mathbb{E}_{c' \sim P(c \mid x)}[\log Q(c' \mid x)]] + H(c)\end{split}\]</div>
<p>This technique of lower bounding mutual information is known as <strong>Variational Information Maximization</strong>.</p>
<p><strong>Lemma 5.1.</strong> For random variables <span class="math notranslate nohighlight">\(X\)</span>, <span class="math notranslate nohighlight">\(Y\)</span> and function <span class="math notranslate nohighlight">\(f(x, y)\)</span> under suitable regularity conditions:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}_{x \sim X, y \sim Y \mid x} [f(x, y)] = \mathbb{E}_{x \sim X, y \sim Y \mid x, x' \sim X \mid x}[f(x', y)]\]</div>
<p>With Lemma 5.1, we can define a variational lower bound <span class="math notranslate nohighlight">\(L_I(G, Q)\)</span> of the mutual information:</p>
<div class="math notranslate nohighlight">
\[\begin{split}L_I(G, Q) &amp; = \mathbb{E}_{c \sim P(c), x \sim G(z, c)}[\log Q(c \mid x)] + H(c) \\
&amp; = \mathbb{E}_{x \sim G(z, c)}[\mathbb{E}_{c' \sim P(c \mid x)}[\log Q(c' \mid x)]] + H(c) \\
&amp; \leq I(c; G(z, c))\end{split}\]</div>
<p>The authors note that <span class="math notranslate nohighlight">\(L_I(G, Q)\)</span> is easy to approximate with Monte Carlo simulation. In particular, <span class="math notranslate nohighlight">\(L_I\)</span> can be maximized w.r.t. <span class="math notranslate nohighlight">\(Q\)</span> directly and w.r.t. <span class="math notranslate nohighlight">\(G\)</span> via the reparameterization trick.</p>
<p>Hence, InfoGAN is defined as the following minimax game:</p>
<div class="math notranslate nohighlight">
\[\min_{G, Q}\max_D V_\text{InfoGAN}(D, G, Q) = V(D, G) - \lambda L_I(G, Q)\]</div>
</div>
<div class="section" id="implementation">
<h2>Implementation<a class="headerlink" href="#implementation" title="Permalink to this headline">¶</a></h2>
<p>In practice, the authors parameterize the auxiliary distribution <span class="math notranslate nohighlight">\(Q\)</span> as a nueral network. <span class="math notranslate nohighlight">\(Q\)</span> and <span class="math notranslate nohighlight">\(D\)</span> share all convolutional layers and there is one final fully connected layer to output parameters for the conditional distribution <span class="math notranslate nohighlight">\(Q(c \mid x)\)</span>.</p>
<p>For categorical latent code <span class="math notranslate nohighlight">\(c_i\)</span>, a softmax nonlinearity is used to represent <span class="math notranslate nohighlight">\(Q(c_i \mid x)\)</span>. For continuous latent code <span class="math notranslate nohighlight">\(c_i\)</span>, treating <span class="math notranslate nohighlight">\(Q(c_i \mid x)\)</span> as a factored Gaussian is sufficient.</p>
</div>
<div class="section" id="experiments">
<h2>Experiments<a class="headerlink" href="#experiments" title="Permalink to this headline">¶</a></h2>
<a class="reference internal image-reference" href="_images/infogan-1.png"><img alt="_images/infogan-1.png" src="_images/infogan-1.png" style="width: 360pt;" /></a>
</div>
</div>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="index.html">Table of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets</a><ul>
<li><a class="reference internal" href="#mutual-information-for-inducing-latent-codes">Mutual Information for Inducing Latent Codes</a></li>
<li><a class="reference internal" href="#variational-mutual-information-maximization">Variational Mutual Information Maximization</a></li>
<li><a class="reference internal" href="#implementation">Implementation</a></li>
<li><a class="reference internal" href="#experiments">Experiments</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="paper-reading/vae.html"
                        title="previous chapter">Auto-Encoding Variational Bayes</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="paper-reading/improved-tech-gans.html"
                        title="next chapter">Improved Techniques for Training GANs</a></p>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/infogan.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="paper-reading/improved-tech-gans.html" title="Improved Techniques for Training GANs"
             >next</a> |</li>
        <li class="right" >
          <a href="paper-reading/vae.html" title="Auto-Encoding Variational Bayes"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">Mofii Notes 1.0.1 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="paper-reading.html" >Paper Reading Notes</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="generative.html" >Generative Models</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets</a></li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2020, Mofii.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 3.1.2.
    </div>
  </body>
</html>