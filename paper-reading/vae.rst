Auto-Encoding Variational Bayes
=====================================

Tutorial on Variational Autoencoders
-------------------------------------

| **Authors:** Carl Doersch
| **Affiliations:** Carnegie Mellon, UC Berkeley

Training generative models has been a long-standing problem and there are three main drawbacks:
  1. they might require strong assumptions about the structure of the data
  2. they might make severe approximations, leading to suboptimal models
  3. they might rely on computationally expensive inference procedures like Markov Chain Monte Carlo

----

| **Authors:** Diederik P. Kingma, Max Welling
| **Affiliations:** Universiteit van Amsterdam

In this work, the authors propose a stochastic variational inference and learning algorithm that performs efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions and large datasets.

Their contributions is two-fold:
  1. They show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods.
  2. They show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model to the intractable posterior using the proposed lower bound estimator.

When a neural network is used for the recognition model, we arrive at the **variational auto-encoder**.

Method
-------------------------------------

The strategy in this section can be used to derive a lower bound estimator (a stochastic objective function) for a variety of directed graphical models with continuous latent variables. We will restrict ourselves here to the common case where we have an i.i.d. dataset with latent variables per datapoint, and where we like to perform maximum likelihood (ML) or maximum a posteriori (MAP) inference on the (global) parameters, and variational inference on the latent variables.

Problem Scenario
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Let us consider some dataset :math:`\mathbf{X} = \{\mathbf{x}^{(i)}\}_{i=1}^N` consisting of :math:`N` i.i.d. samples. We assume that the data are generated by some random process, involving an unobserved continuous random variable :math:`\mathbf{z}`. This process consists of two steps:
  1. a value :math:`\mathbf{z}^{(i)}` is generated from some prior distribution :math:`p_{\mathbf{\theta}^*}(\mathbf{z})`
  2. a value :math:`\mathbf{x}^{(i)}` is generated from some conditional distribution :math:`p_{\mathbf{\theta}^*}(\mathbf{x} \mid \mathbf{z})`
We assume that the prior :math:`p_{\mathbf{\theta}^*}(\mathbf{z})` and likelihood :math:`p_{\mathbf{\theta}^*}(\mathbf{x} \mid \mathbf{z})` come from parametric families of distributions :math:`p_\mathbf{\theta}(\mathbf{z})` and :math:`p_\mathbf{\theta}(\mathbf{x} \mid \mathbf{z})`, and their PDFs are differentiable almost everywhere w.r.t. both :math:`\mathbf{\theta}` and :math:`\mathbf{z}`. This process is hidden and values of :math:`\mathbf{\theta}^*` and :math:`\mathbf{z}^{(i)}` are unknown to us.

The authors do not make common simplifying assumptions about the marginal or posterior probabilities. Conversely, they are interested in a general algorithm that even works efficiently in the case of:
  1. **Intractability:** the case where the integral of the marginal likelihood :math:`p_\mathbf{\theta}(\mathbf{x}) = \int p_\mathbf{\theta}(\mathbf{z})p_\mathbf{\theta}(\mathbf{x} \mid \mathbf{z})d\mathbf{z}` is intractable
  2. **A large dataset:** we have so much data that batch optimization is too costly

Example: Variation Auto-Encoder
-------------------------------------

Experiments
-------------------------------------

Conclusion
-------------------------------------

Future Work
-------------------------------------
